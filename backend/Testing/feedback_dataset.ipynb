{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Comparison on Feedback Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Setup and Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "    roc_curve, auc, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('..')\n",
    "from attention_model import SimpleAttentionCNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Paths and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define paths\n",
    "ORIGINAL_MODEL_PATH = \"../model/tb_chest_xray_attention_best.pt\"\n",
    "FINETUNED_MODEL_PATH = \"../finetuning/tb_chest_xray_refined_20250318_034728.pt\"\n",
    "FEEDBACK_IMG_DIR = \"../feedback/images\"\n",
    "FEEDBACK_CSV = \"../feedback/feedback_log.csv\"\n",
    "\n",
    "# Check if the files and directories exist\n",
    "print(f\"Original model exists: {os.path.exists(ORIGINAL_MODEL_PATH)}\")\n",
    "print(f\"Finetuned model exists: {os.path.exists(FINETUNED_MODEL_PATH)}\")\n",
    "print(f\"Feedback images directory exists: {os.path.exists(FEEDBACK_IMG_DIR)}\")\n",
    "print(f\"Feedback log exists: {os.path.exists(FEEDBACK_CSV)}\")\n",
    "\n",
    "# Load model\n",
    "def load_model(model_path):\n",
    "    model = SimpleAttentionCNN().to(device)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load both models\n",
    "original_model = load_model(ORIGINAL_MODEL_PATH)\n",
    "finetuned_model = load_model(FINETUNED_MODEL_PATH)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "if original_model:\n",
    "    original_model.eval()\n",
    "if finetuned_model:\n",
    "    finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feedback Dataset\n",
    "\n",
    "We'll load the feedback dataset using information from the feedback log CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def load_feedback_dataset():\n",
    "    \"\"\"Load the feedback dataset from feedback_log.csv\"\"\"\n",
    "    if not os.path.isfile(FEEDBACK_CSV) or not os.path.isdir(FEEDBACK_IMG_DIR):\n",
    "        print(f\"Feedback data not found: {FEEDBACK_CSV} or {FEEDBACK_IMG_DIR}\")\n",
    "        return None, None\n",
    "        \n",
    "    try:\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(FEEDBACK_CSV, header=None)\n",
    "        \n",
    "        # Check if we need to add column names\n",
    "        if df.shape[1] >= 4:  # Expecting at least 4 columns\n",
    "            df.columns = ['image_filename', 'mask_filename', 'label', 'timestamp']\n",
    "        elif df.shape[1] == 3:\n",
    "            df.columns = ['image_filename', 'mask_filename', 'label']\n",
    "        \n",
    "        paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            img_file = row['image_filename']\n",
    "            label_str = row['label']\n",
    "            \n",
    "            # Create full path to image\n",
    "            img_path = os.path.join(FEEDBACK_IMG_DIR, os.path.basename(img_file))\n",
    "            \n",
    "            if os.path.isfile(img_path):\n",
    "                # Convert label: TB -> 1, Normal -> 0\n",
    "                label = 1 if label_str.upper() == 'TB' else 0\n",
    "                paths.append(img_path)\n",
    "                labels.append(label)\n",
    "        \n",
    "        print(f\"Loaded {len(paths)} feedback images\")\n",
    "        print(f\"Class distribution: {labels.count(0)} Normal, {labels.count(1)} TB\")\n",
    "        \n",
    "        return paths, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading feedback data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load feedback dataset\n",
    "feedback_paths, feedback_labels = load_feedback_dataset()\n",
    "\n",
    "if feedback_paths is None:\n",
    "    print(\"Failed to load feedback dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Feedback Dataset for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class FeedbackDataset(Dataset):\n",
    "    \"\"\"Dataset class for the feedback images.\"\"\"\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((256, 256)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            # Read the image\n",
    "            img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img_gray is None:\n",
    "                raise FileNotFoundError(f\"Could not read image: {img_path}\")\n",
    "            \n",
    "            # Convert to PIL for transforms\n",
    "            pil_img = Image.fromarray(img_gray, mode='L')\n",
    "            img_tensor = self.transform(pil_img)\n",
    "            \n",
    "            return img_tensor, torch.tensor(label, dtype=torch.float32), img_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "            # Return empty tensor with same shape\n",
    "            return torch.zeros(1, 256, 256), torch.tensor(label, dtype=torch.float32), img_path\n",
    "\n",
    "# Create dataset and dataloader for the feedback data\n",
    "if feedback_paths is not None and len(feedback_paths) > 0:\n",
    "    # Define transforms that match the original model's preprocessing\n",
    "    test_transforms = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    feedback_dataset = FeedbackDataset(feedback_paths, feedback_labels, transform=test_transforms)\n",
    "    feedback_dataloader = DataLoader(feedback_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    print(f\"Created feedback dataset with {len(feedback_dataset)} images\")\n",
    "else:\n",
    "    print(\"Could not create feedback dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model and calculate all metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_attn_maps = []\n",
    "    all_img_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images, labels, paths = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, attention_maps = model(images)\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = outputs.cpu().numpy().flatten()\n",
    "            preds = (outputs >= 0.5).cpu().numpy().flatten().astype(int)\n",
    "            \n",
    "            # Store results\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "            all_attn_maps.extend([attn.cpu().numpy() for attn in attention_maps])\n",
    "            all_img_paths.extend(paths)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n--- {model_name} Performance Metrics on Feedback Dataset ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    results = {\n",
    "        'preds': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probs': all_probs,\n",
    "        'attention_maps': all_attn_maps,\n",
    "        'image_paths': all_img_paths,\n",
    "        'metrics': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': roc_auc,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Both Models on Feedback Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Run evaluation for both models on feedback data\n",
    "model_results = {}\n",
    "\n",
    "if 'feedback_dataloader' in locals() and original_model:\n",
    "    print(\"Evaluating original model on feedback dataset...\")\n",
    "    model_results['original'] = evaluate_model(original_model, feedback_dataloader, \"Original Model\")\n",
    "else:\n",
    "    print(\"Original model or feedback data not available. Cannot evaluate.\")\n",
    "\n",
    "if 'feedback_dataloader' in locals() and finetuned_model:\n",
    "    print(\"\\nEvaluating finetuned model on feedback dataset...\")\n",
    "    model_results['finetuned'] = evaluate_model(finetuned_model, feedback_dataloader, \"Finetuned Model\")\n",
    "else:\n",
    "    print(\"Finetuned model or feedback data not available. Cannot evaluate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(cm, class_names=['Normal', 'Tuberculosis'], title='Confusion Matrix'):\n",
    "    \"\"\"Visualize confusion matrix as a heatmap.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(results_dict):\n",
    "    \"\"\"Plot ROC curves for multiple models on same graph.\"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    colors = {'original': 'blue', 'finetuned': 'red'}\n",
    "    labels = {'original': 'Original Model', 'finetuned': 'Finetuned Model'}\n",
    "    \n",
    "    for model_name, results in results_dict.items():\n",
    "        if model_name in results_dict:\n",
    "            fpr = results['metrics']['fpr']\n",
    "            tpr = results['metrics']['tpr']\n",
    "            roc_auc = results['metrics']['auc']\n",
    "            color = colors.get(model_name, 'green')\n",
    "            label = labels.get(model_name, model_name)\n",
    "            plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                     label=f'{label} (AUC = {roc_auc:.4f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison on Feedback Dataset')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_comparative_metrics_bar_chart(results_dict):\n",
    "    \"\"\"Create side-by-side bar chart comparing metrics between models.\"\"\"\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']\n",
    "    \n",
    "    # Extract metrics for each model\n",
    "    metrics_values = {}\n",
    "    for model_name, results in results_dict.items():\n",
    "        metrics_values[model_name] = [results['metrics'][metric] for metric in metrics_to_plot]\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Set width of bars\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(metric_labels))\n",
    "    \n",
    "    # Create bars\n",
    "    if 'original' in metrics_values:\n",
    "        plt.bar(index - bar_width/2, metrics_values['original'], bar_width, \n",
    "                label='Original Model', color='cornflowerblue', alpha=0.8)\n",
    "        \n",
    "        # Add text labels for original model\n",
    "        for i, v in enumerate(metrics_values['original']):\n",
    "            plt.text(i - bar_width/2 - 0.05, v + 0.01, f'{v:.3f}', \n",
    "                    color='navy', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    if 'finetuned' in metrics_values:\n",
    "        plt.bar(index + bar_width/2, metrics_values['finetuned'], bar_width,\n",
    "                label='Finetuned Model', color='lightcoral', alpha=0.8)\n",
    "        \n",
    "        # Add text labels for finetuned model\n",
    "        for i, v in enumerate(metrics_values['finetuned']):\n",
    "            plt.text(i + bar_width/2 - 0.05, v + 0.01, f'{v:.3f}',\n",
    "                    color='darkred', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison on Feedback Dataset')\n",
    "    plt.xticks(index, metric_labels)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize results for both models\n",
    "if len(model_results) > 0:\n",
    "    # 1. Visualize Confusion Matrices for both models\n",
    "    print(\"\\n--- Confusion Matrix Visualizations ---\")\n",
    "    \n",
    "    if 'original' in model_results:\n",
    "        visualize_confusion_matrix(\n",
    "            model_results['original']['metrics']['confusion_matrix'],\n",
    "            title='Confusion Matrix - Original Model (Feedback Dataset)'\n",
    "        )\n",
    "    \n",
    "    if 'finetuned' in model_results:\n",
    "        visualize_confusion_matrix(\n",
    "            model_results['finetuned']['metrics']['confusion_matrix'],\n",
    "            title='Confusion Matrix - Finetuned Model (Feedback Dataset)'\n",
    "        )\n",
    "    \n",
    "    # 2. Plot comparative ROC Curve\n",
    "    print(\"\\n--- ROC Curves Comparison ---\")\n",
    "    plot_roc_curves(model_results)\n",
    "    \n",
    "    # 3. Plot comparative bar chart of metrics\n",
    "    print(\"\\n--- Performance Metrics Comparison ---\")\n",
    "    plot_comparative_metrics_bar_chart(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis of Prediction Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(results_dict):\n",
    "    \"\"\"Compare prediction differences between models.\"\"\"\n",
    "    if 'original' not in results_dict or 'finetuned' not in results_dict:\n",
    "        print(\"Need both models to compare predictions.\")\n",
    "        return\n",
    "        \n",
    "    orig_preds = results_dict['original']['preds']\n",
    "    ft_preds = results_dict['finetuned']['preds']\n",
    "    labels = results_dict['original']['labels']  # Labels are the same for both models\n",
    "    img_paths = results_dict['original']['image_paths']\n",
    "    \n",
    "    # Find indices where models differ in predictions\n",
    "    diff_indices = [i for i in range(len(orig_preds)) if orig_preds[i] != ft_preds[i]]\n",
    "    \n",
    "    # Classification of differences\n",
    "    improved = [i for i in diff_indices if ft_preds[i] == labels[i] and orig_preds[i] != labels[i]]\n",
    "    worsened = [i for i in diff_indices if ft_preds[i] != labels[i] and orig_preds[i] == labels[i]]\n",
    "    \n",
    "    print(f\"Total feedback samples: {len(orig_preds)}\")\n",
    "    print(f\"Samples where models predict differently: {len(diff_indices)}\")\n",
    "    print(f\"Improvements (finetuned correct, original wrong): {len(improved)}\")\n",
    "    print(f\"Regressions (finetuned wrong, original correct): {len(worsened)}\")\n",
    "    \n",
    "    # Display examples of improvements\n",
    "    if improved:\n",
    "        print(\"\\nExamples of improvements in feedback dataset:\")\n",
    "        plt.figure(figsize=(15, 4*min(3, len(improved))))\n",
    "        \n",
    "        for i, idx in enumerate(improved[:min(3, len(improved))]):\n",
    "            img_path = img_paths[idx]\n",
    "            true_label = \"Tuberculosis\" if labels[idx] == 1 else \"Normal\"\n",
    "            orig_pred = \"Tuberculosis\" if orig_preds[idx] == 1 else \"Normal\"\n",
    "            ft_pred = \"Tuberculosis\" if ft_preds[idx] == 1 else \"Normal\"\n",
    "            \n",
    "            # Load image\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                plt.subplot(min(3, len(improved)), 1, i+1)\n",
    "                plt.imshow(img, cmap='gray')\n",
    "                plt.title(f\"True: {true_label}, Original: {orig_pred}, Finetuned: {ft_pred}\\nFile: {os.path.basename(img_path)}\")\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display examples of regressions\n",
    "    if worsened:\n",
    "        print(\"\\nExamples of regressions in feedback dataset:\")\n",
    "        plt.figure(figsize=(15, 4*min(3, len(worsened))))\n",
    "        \n",
    "        for i, idx in enumerate(worsened[:min(3, len(worsened))]):\n",
    "            img_path = img_paths[idx]\n",
    "            true_label = \"Tuberculosis\" if labels[idx] == 1 else \"Normal\"\n",
    "            orig_pred = \"Tuberculosis\" if orig_preds[idx] == 1 else \"Normal\"\n",
    "            ft_pred = \"Tuberculosis\" if ft_preds[idx] == 1 else \"Normal\"\n",
    "            \n",
    "            # Load image\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                plt.subplot(min(3, len(worsened)), 1, i+1)\n",
    "                plt.imshow(img, cmap='gray')\n",
    "                plt.title(f\"True: {true_label}, Original: {orig_pred}, Finetuned: {ft_pred}\\nFile: {os.path.basename(img_path)}\")\n",
    "                plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Compare predictions between models on the feedback dataset\n",
    "if 'original' in model_results and 'finetuned' in model_results:\n",
    "    print(\"\\n--- Prediction Analysis on Feedback Dataset ---\")\n",
    "    compare_predictions(model_results)\n",
    "else:\n",
    "    print(\"\\nNeed both models to compare predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Generate classification reports for both models\n",
    "if 'original' in model_results:\n",
    "    # Original model report\n",
    "    print(\"\\n--- Original Model Classification Report (Feedback Dataset) ---\")\n",
    "    orig_report = classification_report(\n",
    "        model_results['original']['labels'], \n",
    "        model_results['original']['preds'],\n",
    "        target_names=['Normal', 'Tuberculosis']\n",
    "    )\n",
    "    print(orig_report)\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    orig_report_dict = classification_report(\n",
    "        model_results['original']['labels'],\n",
    "        model_results['original']['preds'],\n",
    "        target_names=['Normal', 'Tuberculosis'],\n",
    "        output_dict=True\n",
    "    )\n",
    "    print(\"Original Model:\")\n",
    "    display(pd.DataFrame(orig_report_dict).transpose().round(4))\n",
    "\n",
    "if 'finetuned' in model_results:\n",
    "    # Finetuned model report\n",
    "    print(\"\\n--- Finetuned Model Classification Report (Feedback Dataset) ---\")\n",
    "    ft_report = classification_report(\n",
    "        model_results['finetuned']['labels'], \n",
    "        model_results['finetuned']['preds'],\n",
    "        target_names=['Normal', 'Tuberculosis']\n",
    "    )\n",
    "    print(ft_report)\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    ft_report_dict = classification_report(\n",
    "        model_results['finetuned']['labels'],\n",
    "        model_results['finetuned']['preds'],\n",
    "        target_names=['Normal', 'Tuberculosis'],\n",
    "        output_dict=True\n",
    "    )\n",
    "    print(\"Finetuned Model:\")\n",
    "    display(pd.DataFrame(ft_report_dict).transpose().round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
