{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "DEBUG = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3,\n",
    "            stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3,\n",
    "            stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_blocks=[2,2,2,2], num_classes=2, in_channels=1):\n",
    "        super(MyResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, 64, kernel_size=7, stride=2,\n",
    "            padding=3, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64,  num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, out_channels, stride=stride))\n",
    "        self.in_planes = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.pool1(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.forward_features(x)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def build_model():\n",
    "    model = MyResNet(\n",
    "        block=BasicBlock,\n",
    "        num_blocks=[2,2,2,2],  \n",
    "        num_classes=2,\n",
    "        in_channels=1  \n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_alb = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.RandomRotate90(p=0.2),            \n",
    "    A.HorizontalFlip(p=0.5),            \n",
    "    A.RandomBrightnessContrast(p=0.2),  \n",
    "    A.Normalize(mean=(0.5,), std=(0.25,)),  \n",
    "    ToTensorV2()\n",
    "],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=['class_labels']\n",
    "    )\n",
    ")\n",
    "\n",
    "val_transform_alb = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "    ToTensorV2()\n",
    "],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        label_fields=['class_labels']\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PneumoniaDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None, is_train=True):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        grouped = self.df.groupby('patientId')\n",
    "        self.records = []\n",
    "        for pid, group in grouped:\n",
    "            target_vals = group['Target'].values\n",
    "            label = int(np.any(target_vals == 1))\n",
    "\n",
    "            bboxes = []\n",
    "            for idx, row in group.iterrows():\n",
    "                if row['Target'] == 1:\n",
    "                    x, y, w, h = row['x'], row['y'], row['width'], row['height']\n",
    "                    x_min, y_min = x, y\n",
    "                    x_max, y_max = x + w, y + h\n",
    "                    bboxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "            self.records.append({\n",
    "                'patientId': pid,\n",
    "                'label': label,\n",
    "                'bboxes': bboxes\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        pid = record['patientId']\n",
    "        label = record['label']\n",
    "        bboxes = record['bboxes']  \n",
    "\n",
    "        img_path = os.path.join(self.img_dir, pid + '.png')\n",
    "        image_pil = Image.open(img_path).convert('L')  \n",
    "        image_np = np.array(image_pil)  \n",
    "\n",
    "\n",
    "        image_np = np.expand_dims(image_np, axis=-1)  \n",
    "\n",
    "        class_labels = [1]*len(bboxes)  \n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image_np,\n",
    "                bboxes=bboxes,\n",
    "                class_labels=class_labels\n",
    "            )\n",
    "            aug_image = transformed['image']           \n",
    "            aug_bboxes = transformed['bboxes']         \n",
    "        else:\n",
    "            aug_image = torch.tensor(image_np).permute(2,0,1).float()/255.0\n",
    "            aug_bboxes = bboxes\n",
    "        final_bboxes = []\n",
    "        for (x_min, y_min, x_max, y_max) in aug_bboxes:\n",
    "            w = x_max - x_min\n",
    "            h = y_max - y_min\n",
    "            final_bboxes.append((x_min, y_min, w, h))\n",
    "\n",
    "        return aug_image, label, final_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(model, input_tensor, class_idx=1):\n",
    "    model.eval()  \n",
    "    conv_features = []\n",
    "    conv_grads = []\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        conv_features.append(output)\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        conv_grads.append(grad_output[0])\n",
    "    \n",
    "    forward_handle = model.layer4.register_forward_hook(forward_hook)\n",
    "    backward_handle = model.layer4.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    logits = model(input_tensor)  \n",
    "    chosen_logit = logits[:, class_idx].sum()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    chosen_logit.backward(retain_graph=True)\n",
    "    \n",
    "    features = conv_features[0]  \n",
    "    grads = conv_grads[0]      \n",
    "    \n",
    "    weights = grads.view(grads.size(0), grads.size(1), -1).mean(dim=2)  \n",
    "    \n",
    "    B, C, H, W = features.shape\n",
    "    gradcam = torch.zeros((B, 1, H, W), device=features.device)\n",
    "    for i in range(B):\n",
    "        for c in range(C):\n",
    "            gradcam[i, 0] += weights[i, c] * features[i, c]\n",
    "    \n",
    "    gradcam = torch.relu(gradcam)\n",
    "    max_vals = gradcam.view(B, -1).max(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "    gradcam = gradcam / (max_vals + 1e-8) \n",
    "    \n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "    \n",
    "    model.train()  \n",
    "    return gradcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes_to_mask(bboxes, img_size=(224,224), feature_size=(7,7)):\n",
    "    mask = np.zeros(feature_size, dtype=np.float32)\n",
    "\n",
    "    scale_x = feature_size[1] / img_size[1]\n",
    "    scale_y = feature_size[0] / img_size[0]\n",
    "\n",
    "    for (x, y, w, h) in bboxes:\n",
    "        x1 = int(x * scale_x)\n",
    "        y1 = int(y * scale_y)\n",
    "        x2 = int((x + w) * scale_x)\n",
    "        y2 = int((y + h) * scale_y)\n",
    "\n",
    "        x1, x2 = max(0, x1), min(feature_size[1], x2)\n",
    "        y1, y2 = max(0, y1), min(feature_size[0], y2)\n",
    "\n",
    "        mask[y1:y2, x1:x2] = 1.0\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def alignment_loss(gradcam, bbox_mask):\n",
    "    eps = 1e-6\n",
    "    B, _, H, W = gradcam.shape\n",
    "    cam_flat = gradcam.view(B, -1)\n",
    "    mask_flat = bbox_mask.view(B, -1)\n",
    "\n",
    "    overlap = torch.sum(cam_flat * mask_flat, dim=1)\n",
    "    sum_cam = torch.sum(cam_flat, dim=1) + eps\n",
    "    ratio = overlap / sum_cam  \n",
    "    loss = 1.0 - ratio         \n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, alpha=0.1, device='cuda'):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    from tqdm.notebook import tqdm  \n",
    "    import numpy as np\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", unit=\"batch\")\n",
    "    \n",
    "    for batch_idx, (images, labels, bboxes) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(images) \n",
    "        ce_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        if DEBUG:\n",
    "            print(f\"Batch {batch_idx}: CE Loss = {ce_loss.item():.4f}\")\n",
    "        \n",
    "        gradcam = compute_gradcam(model, images, class_idx=1)  \n",
    "        B, _, Hf, Wf = gradcam.shape\n",
    "        if DEBUG:\n",
    "            print(f\"Batch {batch_idx}: Grad-CAM shape = {gradcam.shape}\")\n",
    "        \n",
    "        masks_list = []\n",
    "        for i in range(B):\n",
    "            if labels[i] == 1:\n",
    "                mask_np = bboxes_to_mask(bboxes[i], img_size=(224,224), feature_size=(Hf, Wf))\n",
    "            else:\n",
    "                mask_np = np.zeros((Hf, Wf), dtype=np.float32)\n",
    "            masks_list.append(mask_np)\n",
    "        \n",
    "        bbox_mask_np = np.array(masks_list)  \n",
    "        bbox_mask_tensor = torch.from_numpy(bbox_mask_np).to(device).unsqueeze(1)  \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(f\"Batch {batch_idx}: BBox Mask tensor shape = {bbox_mask_tensor.shape}, sum = {torch.sum(bbox_mask_tensor).item()}\")\n",
    "        \n",
    "\n",
    "        align_loss = alignment_loss(gradcam, bbox_mask_tensor)\n",
    "        if DEBUG:\n",
    "            print(f\"Batch {batch_idx}: Alignment Loss = {align_loss.item():.4f}\")\n",
    "        \n",
    "\n",
    "        total_loss = ce_loss + alpha * align_loss\n",
    "        if DEBUG:\n",
    "            print(f\"Batch {batch_idx}: Total Loss = {total_loss.item():.4f}\")\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"CE Loss\": f\"{ce_loss.item():.4f}\",\n",
    "            \"Align\": f\"{align_loss.item():.4f}\",\n",
    "            \"Total\": f\"{total_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(f\"Batch {batch_idx} complete: CE Loss = {ce_loss.item():.4f}, \"\n",
    "                  f\"Align Loss = {align_loss.item():.4f}, Total Loss = {total_loss.item():.4f}\\n\")\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=5, alpha=0.1, lr=1e-3, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"=== EPOCH {epoch+1}/{num_epochs} ===\")\n",
    "        epoch_loss, epoch_acc = train_one_epoch(model, train_loader, optimizer, alpha=alpha, device=device)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%\\n\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    all_bboxes = []\n",
    "    for (img, lbl, bxs) in batch:\n",
    "        images.append(img)\n",
    "        labels.append(lbl)\n",
    "        all_bboxes.append(bxs)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return images, labels, all_bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV path: f:\\Final Year\\Final Project\\Clinician-Guided-Grad-CAM\\dataset\\stage2_train_metadata.csv\n",
      "Train image directory: f:\\Final Year\\Final Project\\Clinician-Guided-Grad-CAM\\dataset\\Training\\Images\n",
      "=== EPOCH 1/5 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a046c84284710aa0d5342c9067955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3336 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    from tqdm.notebook import tqdm  \n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    base_dir = os.path.abspath(\".\")\n",
    "    train_csv_path = os.path.join(base_dir, \"dataset\", \"stage2_train_metadata.csv\")\n",
    "    train_img_dir = os.path.join(base_dir, \"dataset\", \"Training\", \"Images\")\n",
    "    \n",
    "    print(\"Train CSV path:\", train_csv_path, flush=True)\n",
    "    print(\"Train image directory:\", train_img_dir, flush=True)\n",
    "    \n",
    "    train_dataset = PneumoniaDataset(\n",
    "        csv_path=train_csv_path,\n",
    "        img_dir=train_img_dir,\n",
    "        transform=train_transform_alb,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        num_epochs=5,\n",
    "        alpha=0.05,   \n",
    "        lr=1e-4,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
